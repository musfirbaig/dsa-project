import json
import os
import sys
import time
from nltk.stem import SnowballStemmer
from constants import STOP_WORDS
import ujson
import shutil
import math
import itertools
STOP_WORDS_SET = set(STOP_WORDS)
stemmer = SnowballStemmer("english")
# this hashing function was written by me specifically for nela-gt-2022 dataset,
# it sufficiently (uniformaly) distributes words in the search engine barrels 
class Hashing:
    def HasherFunction(self, inputString):
        sum = 0
        for index, element in enumerate(inputString):
            sum = sum + ((len(inputString) - index) * ord(element))
        return (sum)%500
    
# class written to generate forward index 
class ForwardIndex:

    def convertToWords(self, content):
        """
        INPUT: article content
        OUTPUT: list of words for case insensitivity (lowercase)
        """
        words = content.lower().split(' ')
        words = [stemmer.stem(word).encode("ascii", errors="ignore").decode().strip('\',._+/\\!@#$?^()[]}{"').strip() for word in words if word not in STOP_WORDS_SET]
        words = [word for word in words if len(word) != 0]
        return words


    def dictWordToPositionAndFrequency(self, words):
        """
        INPUT: list of words and docID,
        OUTPUT: Hash Map of objects containing word-> freq and docID-> docID
        """
        wordsObject = {}
        for index, word in enumerate(words):
            if word in wordsObject:
                wordsObject[word]["freq"] += 1
                wordsObject[word]["pos"].append(index)
            else:
                wordsObject[word] = {
                    "freq" : 1,
                    "pos" : [index]
                }
        return wordsObject


    def createForwardIndex(self, documentList):
        """
        INPUT: A list of .json files in a directory
        OUTPUT: The Corresponding Forward Index
        """

        self.__noOfArticles = 0
        os.system("cls")
        print("generating forward index....")
        start_time = time.time()
        forwardIndex = []
        keysOfArticle = list(documentList[0][0].keys())
        keysOfArticle.remove("content")
        for jsonDocument in documentList:
            for article in jsonDocument:
                articleObject = {"metaData": {}}
                words = self.convertToWords(article["content"])
                wordsObject = self.dictWordToPositionAndFrequency(words)
                articleObject["words"] = wordsObject
                for key in keysOfArticle:
                    articleObject["metaData"][key] = article[key]
                forwardIndex.append(articleObject)
                if sys.getsizeof(forwardIndex) >= 50000:
                    self.writeForwardIndexToFile(forwardIndex)
                    forwardIndex = []
                self.__noOfArticles += 1
        if len(forwardIndex):
            self.writeForwardIndexToFile(forwardIndex)
        os.system("cls")
        print("Execution Time (Forward Index): " + str(time.time()-start_time))
        time.sleep(2)
    
    def documentList(self, directoryName):
        """
        INPUT: Directory to operate on
        OUTPUT: List of files in directory
        """
        files = os.listdir(directoryName)
        files = [(directoryName+"/"+f) for f in files if os.path.isfile(directoryName+'/'+f)]
        self.__fileCounter = 0
        listDr = []
        for file in files:
            with open(file) as f:
                listDr.append(ujson.load(f))
        return listDr


    def forwardIndexGenerator(self, directoryName):
        """
        INPUT: Directory to operate on
        OUTPUT: Corresponding Forward Index
        """
        documentList = self.documentList(directoryName)
        self.createForwardIndex(documentList)    

    def writeForwardIndexToFile(self, forwardIndex):
        """
        INPUT: Forward Index
        SIDE EFFECTS: Dumping Forward Index to File
        """
        
        os.makedirs("Forward_Index", exist_ok=True)
        file_path = "Forward_index/" + "stemmedIndex" + str(self.__fileCounter) + ".json"
        print("writing(): " + file_path)
        self.__fileCounter += 1
        with open(file_path, 'w') as json_file:
            ujson.dump(forwardIndex, json_file)

    def getNoFiles(self):
        return self.__fileCounter
    
    def noProcessedArticles(self):
        return self.__noOfArticles
# class operates on the ForwardIndex generated by the previous class and generates
# barrel based inverted index
class MetadataFile:
    def createMetadataJSON(self, documentList):
        """
        INPUT: List of document metadata (e.g., URLs, docIDs)
        OUTPUT: JSON file with URLs mapped to docIDs
        """
        metadata = []  # List to store metadata dictionaries

        for jsonDocument in documentList:
            for article in jsonDocument:
                doc_id = article['id']
                url = article['url']  # Assuming 'url' is the key for URLs in the JSON

                # Create a dictionary representing metadata for each article
                metadata.append({
                    'doc_id': doc_id,
                    'url': url
                })

        # Define the filename for the JSON file
        json_filename = "metadata_urls.json"

        # Write the metadata list to a JSON file
        with open(json_filename, 'w') as json_file:
            json.dump(metadata, json_file, indent=4)  # The indent parameter adds formatting for readability

class InvertedIndex:

    def writeAndIfUpdateBarrel(self, barrel: dict, barrelName: str):
        
        path = f"Inverted_Index/barrel{barrelName}.json"
        temp = {}
        
        if os.path.exists(path):
            with open(path, mode="r") as alreadyBarrel:
                temp = ujson.load(alreadyBarrel)
        if len(temp) == 0:
            print("writing(): " + path)
        else:
            print("updating(): " + path)
        with open(path, mode="w") as writeFile:
            for word, info in barrel.items():
                if word in temp:
                    # Update existing information if article IDs match
                    temp[word].append(info)
                else:
                    # Add completely new word information
                    temp[word] = info
            ujson.dump(temp, writeFile)
        
                
    def calculateTFIDF(self, freq, totalDocs, docsWithTerm):
        tf = freq / totalDocs
        idf = math.log(totalDocs / (1 + docsWithTerm))
        return tf * idf

    def generateInvertedIndex(self, noFiles):
        """
        SIDE EFFECTS: generates a inverted index for the
        forward index present in the directory
        """
        if os.path.exists("Inverted_Index"):
            shutil.rmtree("Inverted_Index")
        os.system("cls")
        print("generating inverted index....")
        os.makedirs("Inverted_Index", exist_ok=True)
        hashingObject = Hashing()
        startTime = time.time()
        invertedIndex = {}
        totalDocs = noFiles  # Move this line to the outside of the loop

        for indexNo in range(noFiles):
            with open("Forward_Index/stemmedIndex" + str(indexNo) + ".json", "r") as forwardIndexFile:
                forwardIndex = ujson.load(forwardIndexFile)
            for articleData in forwardIndex:
                words = articleData["words"].keys()
                for word in words:
                    barrelName = hashingObject.HasherFunction(word)
                    wordObjectInfo = articleData["words"][word]
                    articleId = articleData["metaData"]["id"]
                    wordObjectInfo["id"] = articleId

                    # Check if barrelName exists in invertedIndex
                    if barrelName not in invertedIndex:
                        invertedIndex[barrelName] = {}

                    docsWithTerm = len(invertedIndex[barrelName].get(word, []))
                    rank = self.calculateTFIDF(wordObjectInfo["freq"], totalDocs, docsWithTerm)
                    wordObjectInfo["rank"] = rank

                    if word in invertedIndex[barrelName]:
                        invertedIndex[barrelName][word].append(wordObjectInfo)
                    else:
                        invertedIndex[barrelName][word] = [wordObjectInfo]

                    if sys.getsizeof(invertedIndex[barrelName]) > 7000:
                        self.writeAndIfUpdateBarrel(invertedIndex[barrelName], barrelName)
                        invertedIndex[barrelName] = {}

        barrelNames = invertedIndex.keys()
        forwardIndex = []
        for barrelName in barrelNames:
            self.writeAndIfUpdateBarrel(invertedIndex[barrelName], barrelName)
        os.system("cls")
        print("Execution Time (Inverted Index): " + str(time.time() - startTime))
        time.sleep(2)
    def searchSingleWord(self, query):
        """
        Search the inverted index for information related to a single word.
        Sorts document IDs based on their rank and retrieves corresponding URLs.
        """

        hashingObject = Hashing()
        stemmed_query = stemmer.stem(query)
        barrelName = hashingObject.HasherFunction(stemmed_query)
        urls = []

        # Load the corresponding barrel
        barrelPath = f"Inverted_Index/barrel{barrelName}.json"
        if not os.path.exists(barrelPath):
            print(f"No information available for the word '{query}'.")
            return []

        with open(barrelPath, mode="r") as barrelFile:
            barrel = ujson.load(barrelFile)

        # Check if the query word exists in the barrel
        if stemmed_query in barrel:
            wordInfo = barrel[stemmed_query]
            # Assuming wordInfo contains the data you provided
            flattened_word_info = [item for sublist in wordInfo for item in (sublist if isinstance(sublist, list) else [sublist])]
            sorted_word_info = sorted(flattened_word_info, key=lambda x: x.get('rank', 0) if isinstance(x, dict) else 0, reverse=True)

  # Sort based on rank
            doc_ids = [info['id'] for info in sorted_word_info if isinstance(info, dict)]
            # Retrieve URLs corresponding to document IDs from metadata file
            metadata = []
            with open("metadata_urls.json", "r") as metadata_file:
                metadata = json.load(metadata_file)

            # Retrieve URLs for the sorted document IDs
            for doc_id in doc_ids:
                for data in metadata:
                    if data['doc_id'] == doc_id:
                        urls.append(data['url'])
                        break

        else:
            print(f"No information available for the word '{query}' in the current barrel.")

        return urls

    def searchMultiWord(self, query):
        hashingObject = Hashing()
        query_words = query.split()
        doc_ids_per_word = []
        stemmed_query_words = [stemmer.stem(word) for word in query_words]  # Stemming each query word

        for word in stemmed_query_words:
            barrelName = hashingObject.HasherFunction(word)
            barrelPath = f"Inverted_Index/barrel{barrelName}.json"
            if os.path.exists(barrelPath):
                with open(barrelPath, mode="r") as barrelFile:
                    barrel = ujson.load(barrelFile)

                    if word in barrel:
                        wordInfo = barrel[word]
                        flattened_word_info = [item for sublist in wordInfo for item in (sublist if isinstance(sublist, list) else [sublist])]
                        sorted_word_info = sorted(flattened_word_info, key=lambda x: x.get('rank', 0) if isinstance(x, dict) else 0, reverse=True)

  # Sort based on rank
                        doc_ids = [info['id'] for info in sorted_word_info if isinstance(info, dict)]
                        doc_ids_per_word.append(set(doc_ids))
        if doc_ids_per_word:
            # Check for common documents for all words
            common_doc_ids = list(set.intersection(*doc_ids_per_word))
            
            if common_doc_ids:
                metadata = []
                with open("metadata_urls.json", "r") as metadata_file:
                    metadata = json.load(metadata_file)

                urls = [data['url'] for data in metadata if data['doc_id'] in common_doc_ids]
                return urls

            # Check for common documents for pairs of words for wxample if there are five words and if there are no common element between five of them then it checks wether 4 words have anything in common
            for subset_size in range(len(query_words) - 1, 1, -1):  # Iterate over subsets
                subsets = itertools.combinations(doc_ids_per_word, subset_size)
                for subset in subsets:
                    common_subset_ids = list(set.intersection(*subset))
                    if common_subset_ids:
                        metadata = []
                        with open("metadata_urls.json", "r") as metadata_file:
                            metadata = json.load(metadata_file)

                        urls = [data['url'] for data in metadata if data['doc_id'] in common_subset_ids]

                        return urls
        return []
